{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c7cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset, load_from_disk\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel, MambaConfig\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, DataCollatorForLanguageModeling, PretrainedConfig, MambaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c7822d-d877-4b0f-a379-3701339499b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 4096\n",
    "DATASET_PATH = './data/tokenized_dataset_130g_4k_v2/'\n",
    "TOKENIZER_PATH = './tokenizer/tokenizer_v4/'\n",
    "MODEL_PATH = './model/komamba_deepspeed_130m_130g_2k_2k'\n",
    "OUTPUT_PATH = './output_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638308c2-ef28-405e-9c39-80cd50848ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e826a5de-3525-4cbd-bf73-13f711e497a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = load_from_disk(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f93ad5-678e-4b4f-8423-11f7f8289afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 4772510\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 531549\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06081780-57f2-4a39-a320-0920d9b1157f",
   "metadata": {},
   "source": [
    "model_path = 'state-spaces/mamba-370m'\n",
    "\n",
    "xmodel = MambaLMHeadModel.from_pretrained(model_path, dtype=torch.float16)\n",
    "\n",
    "conf = xmodel.config\n",
    "\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eff272",
   "metadata": {},
   "source": [
    "\n",
    "conf = MambaConfig(d_model=768,\n",
    "                   n_layer=24,\n",
    "                   vocab_size=51200,\n",
    "                   ssm_cfg={},\n",
    "                   rms_norm=True,\n",
    "                   residual_in_fp32=True,\n",
    "                   fused_add_norm=True,\n",
    "                   pad_vocab_size_multiple=8)\n",
    "\n",
    "mamba-370m\n",
    "\n",
    "MambaConfig(d_model=1024, n_layer=48, vocab_size=50277, ssm_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8)\n",
    "\n",
    "mamba-790m\n",
    "\n",
    "MambaConfig(d_model=1536, n_layer=48, vocab_size=50277, ssm_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8)\n",
    "\n",
    "\n",
    "mamba-1.4b\n",
    "\n",
    "MambaConfig(d_model=2048, n_layer=48, vocab_size=50277, ssm_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8)\n",
    "\n",
    "\n",
    "mamba-2.8b\n",
    "\n",
    "MambaConfig(d_model=2560, n_layer=64, vocab_size=50277, ssm_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c165adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = MambaConfig(d_model=768,\n",
    "                   n_layer=24,\n",
    "                   vocab_size=51200,\n",
    "                   ssm_cfg={},\n",
    "                   rms_norm=True,\n",
    "                   residual_in_fp32=True,\n",
    "                   fused_add_norm=True,\n",
    "                   pad_vocab_size_multiple=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbbea058-7bb1-46f8-987e-972763ce09da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaConfig(d_model=768, n_layer=24, vocab_size=51200, ssm_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d41e011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaLMHeadModel(config=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4115da84-e91e-40ea-b960-5016751ad3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\"d_model\": 768, \n",
    "        \"n_layer\": 24, \n",
    "        \"vocab_size\": 51200, \n",
    "        \"ssm_cfg\": {}, \n",
    "        \"rms_norm\": True, \n",
    "        \"residual_in_fp32\": True, \n",
    "        \"fused_add_norm\": True, \n",
    "        \"pad_vocab_size_multiple\": 8}\n",
    "\n",
    "model.config = PretrainedConfig(**{**conf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8990e174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaLMHeadModel(\n",
       "  (backbone): MixerModel(\n",
       "    (embedding): Embedding(51200, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (mixer): Mamba(\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2c2fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3345ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a432501-7fd7-4e67-9808-d183eba7c11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657b03d-622a-4582-86fd-30a53e141281",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a169adc3-7901-4383-a5aa-c309ed0838a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, device, prompt, max_length=20):\n",
    "    \"\"\"Generate text using given prompt\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt')[\"input_ids\"].to(device=device)\n",
    "    out = model.generate(input_ids, max_new_tokens=max_length)\n",
    "    return tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "796ab29f-34a9-4863-ac19-8b2face6059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, device, prompt, max_length=20):\n",
    "    \"\"\"Generate text using given prompt\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids.to(device=device)\n",
    "    attn_mask = inputs.attention_mask.to(device=device)\n",
    "\n",
    "    generate_ids = model.generate(input_ids=input_ids,\n",
    "                                  max_length=input_ids.shape[1]+max_length,\n",
    "                                  cg=True,\n",
    "                                  return_dict_in_generate=True,\n",
    "                                  output_scores=True,\n",
    "                                  enable_timing=False,\n",
    "                                  temperature=1.0,\n",
    "                                  top_k=1,\n",
    "                                  top_p=1.0,\n",
    "                                  repetition_penalty=1.0)\n",
    "    generate_ids.sequences\n",
    "\n",
    "    return tokenizer.batch_decode(generate_ids.sequences, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73281018-d266-437f-89a4-4454c7e7284c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을 행동을'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을\"\n",
    "out = generate(model, tokenizer, device, prompt)\n",
    "str(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42d9afa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([3, 4098])\n",
      "attention_mask: torch.Size([3, 4098])\n",
      "labels: torch.Size([3, 4098])\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "out = data_collator([tokenized_datasets['train'][i] for i in range(3)])\n",
    "\n",
    "for key in out:\n",
    "    print(f\"{key}: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc7b6ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  522,   483,   483,  6667,  7665, 10485, 33700, 10486, 34038,  2346,\n",
       "          6451, 22681,  6187,  6954,  6490, 15479,  3753, 28367,  6259,  1185]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor([  522,   483,   483,  6667,  7665, 10485, 33700, 10486, 34038,  2346,\n",
       "          6451, 22681,  6187,  6954,  6490, 15479,  3753, 28367,  6259,  1185]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][0][:20], out['attention_mask'][0][:20], out['labels'][0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ddb8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size=16\n",
    "logging_steps=1000\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"kowikimamba\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_steps=logging_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    gradient_accumulation_steps=8,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=logging_steps,\n",
    "    max_steps=7000,\n",
    "    #num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    save_steps=5000,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2afed41b-01bf-4671-ace0-91d45d0a72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class MambaTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids = inputs.pop(\"input_ids\")\n",
    "        lm_logits = model(input_ids).logits\n",
    "\n",
    "        labels = input_ids.to(lm_logits.device)\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return lm_loss\n",
    "\n",
    "    def save_model(self, output_dir, _internal_call):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        torch.save(self.model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
    "        self.tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fab85ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MambaTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "935499d5-1b83-4ae6-acb9-f9c19dcc8620",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 25.01 GiB. GPU 0 has a total capacty of 39.39 GiB of which 8.60 GiB is free. Including non-PyTorch memory, this process has 30.79 GiB memory in use. Of the allocated memory 28.11 GiB is allocated by PyTorch, and 522.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/transformers/trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1938\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1940\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1943\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1946\u001b[0m ):\n\u001b[1;32m   1947\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m, in \u001b[0;36mMambaTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     lm_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      8\u001b[0m     labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m     shift_logits \u001b[38;5;241m=\u001b[39m lm_logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:203\u001b[0m, in \u001b[0;36mDataParallel.gather\u001b[0;34m(self, outputs, output_device)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgather\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: Any, output_device: Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:105\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(outputs, target_device, dim)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Recursive function calls like this create reference cycles.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Setting the function to None clears the refcycle.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mgather_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     gather_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:99\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)((k, gather_map([d[k] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m outputs]))\n\u001b[1;32m     97\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m out)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(out):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgather_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(out)(\u001b[38;5;28mmap\u001b[39m(gather_map, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/collections/__init__.py:402\u001b[0m, in \u001b[0;36mnamedtuple.<locals>._make\u001b[0;34m(cls, iterable)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make\u001b[39m(\u001b[38;5;28mcls\u001b[39m, iterable):\n\u001b[0;32m--> 402\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtuple_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _len(result) \u001b[38;5;241m!=\u001b[39m num_fields:\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fields\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m arguments, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:90\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     88\u001b[0m out \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGather\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:75\u001b[0m, in \u001b[0;36mGather.forward\u001b[0;34m(ctx, target_device, dim, *inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     ctx\u001b[38;5;241m.\u001b[39munsqueezed_scalar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     74\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(i\u001b[38;5;241m.\u001b[39msize(ctx\u001b[38;5;241m.\u001b[39mdim) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/parallel/comm.py:231\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(tensors, dim, destination, out)\u001b[0m\n\u001b[1;32m    227\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    228\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing -1 to represent CPU tensor is deprecated. Please use a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice object or string instead, e.g., \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    230\u001b[0m     destination \u001b[38;5;241m=\u001b[39m _get_device_index(destination, allow_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m destination \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 25.01 GiB. GPU 0 has a total capacty of 39.39 GiB of which 8.60 GiB is free. Including non-PyTorch memory, this process has 30.79 GiB memory in use. Of the allocated memory 28.11 GiB is allocated by PyTorch, and 522.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6fd845-5d4d-4ef2-a03d-066e9e58ec34",
   "metadata": {},
   "source": [
    "(1) 790m, 77g, 10k steps, 19.33 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7fe3a8d-29ad-4b43-acae-d4ab8bc0c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./model/komamba_790m_7k_f16_77g\", ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "915fd267-7b4b-4d2b-9f44-379cf4f077ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_model() missing 1 required positional argument: '_internal_call'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model/komamba_790m_7k_f16_77g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "\u001b[0;31mTypeError\u001b[0m: save_model() missing 1 required positional argument: '_internal_call'"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./model/komamba_790m_7k_f16_77g\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c5a1cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 취하는 것을 거부했다.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nerate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3250fc",
   "metadata": {},
   "source": [
    "(1) '카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 할 수 있는 능력을 가진 것으로 생각한다고 말했다.'\n",
    "\n",
    "(2) '카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 취해야 한다고 주장했다.'\n",
    "\n",
    "(3) '카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 취해야 한다고 주장했다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0be020fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'활동은 소설, 극작, 수필, 번역, 라디오 드라마와 영화 시나리오 등 다양하다. 1974년부터 1991년까지 공산당에 입당하고 있으며, 초기에는 막시즘 관점에서 활동했다.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"활동은 소설, 극작, 수필, 번역, 라디오 드라마와 영화 시나리오 등 다양하다. 1974년부터 1991년까지 공산당에 입당하고 있으며, 초기에는 막시즘 관점에서\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,python zero_to_fp32.py . pytorch_model.bin\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36585585",
   "metadata": {},
   "source": [
    "(1) '활동은 소설, 극작, 수필, 번역, 라디오 드라마와 영화 시나리오 등 다양하다. 1974년부터 1991년까지 공산당에 입당하고 있으며, 초기에는 막시즘 관점에서 공산주의의 이념과 이념의 변화를 강조하면서 정치·경제·정치·예술 등 다양한 분야에서 활동했다.'\n",
    "\n",
    "(2) '활동은 소설, 극작, 수필, 번역, 라디오 드라마와 영화 시나리오 등 다양하다. 1974년부터 1991년까지 공산당에 입당하고 있으며, 초기에는 막시즘 관점에서 사회 개혁 운동을 주도했으며, 특히 제2차 세계 대전 당시 나치 독일의 나치당 지도자였던 아돌프 히틀러에 의해 나치당에 가담하였다.'\n",
    "\n",
    "(3) '활동은 소설, 극작, 수필, 번역, 라디오 드라마와 영화 시나리오 등 다양하다. 1974년부터 1991년까지 공산당에 입당하고 있으며, 초기에는 막시즘 관점에서만 활동했으나 현재는 중도좌파 성향의 정당이다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b13562d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'근육이 커지기 위해서는 근육이 필요하다. 근육이 근육을 만들어주는 것은 근육이기 때문이다. 근육이 근육을 만들어주는 것은 근육이 근육을 만들어주는 것과 같다. 근육이 근육을 만들어주는 것은 근육을 만들어주는 것이다. 근육이 근육을 만들어주는 것은 근육을 만들어주는 것이다.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"근육이 커지기 위해서는\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5fad79",
   "metadata": {},
   "source": [
    "(1) '근육이 커지기 위해서는, 운동하는 운동 운동이나 운동 운동이나 운동 운동에도 사용된다.'\n",
    "\n",
    "(2) '근육이 커지기 위해서는, 이 두 가지 중 어느 하나라도 더하면 안 되는 것이다.'\n",
    "\n",
    "(3) '근육이 커지기 위해서는, 근육이나 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육, 근육'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df63786b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'피부의 조직이 괴사된다면, 피부는 피부의 조직을 파괴하는 것이 아니라 피부의 조직을 파괴하는 것이다.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"피부의 조직이 괴사된다면\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305fae2",
   "metadata": {},
   "source": [
    "(1) '피부의 조직이 괴사된다면 뇌가 뇌에 침투하여 뇌에 손상을 입히거나 손상되지 않도록 한다.'\n",
    "\n",
    "(2) '피부의 조직이 괴사된다면 근육과 근육 사이의 근육 관계는 점점 더 악화된다.'\n",
    "\n",
    "(3) '피부의 조직이 괴사된다면 그 근육은 근육을 자극하여 근육을 자극하게 된다.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b73ab",
   "metadata": {},
   "source": [
    "(1) 130m, kowiki, batch_size=48, 10 epochs, 21.35h\n",
    "\n",
    "(2) 370m, kowiki, batch_size=48, 10000 steps, 28.40h \n",
    "\n",
    "(3) 790m, kowiki, batch_size=16, f16, 10000 steps, 28.40h \n",
    "\n",
    "(4) 1.3b, kowiki, batch_size=, steps,\n",
    "\n",
    "(5) 2.8b, kowiki, batch_size=, steps,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ebe8290-dd72-4a6f-9a82-e5a0a1adaaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
    "\n",
    "class MambaLMHeadModel2(MambaLMHeadModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: MambaConfig,\n",
    "        initializer_cfg=None,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ) -> None:\n",
    "        super().__init__(config, initializer_cfg, device, dtype)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):\n",
    "        config = load_config_hf(pretrained_model_name)\n",
    "        model = cls(**config, device=device, dtype=dtype, **kwargs)\n",
    "        model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))\n",
    "        return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe23113-5d6b-48a8-89fd-3df6985801d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0622a4-ab0c-4802-bad5-777ac4b8a138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5caec20-03cb-4a33-87cd-60733e2d5003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e41737-f6d6-4847-a85b-669b98218106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the saved model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Load the saved model\\n')\n",
    "#new_model = MambaLMHeadModel.from_pretrained('./model', dtype=torch.float16)\n",
    "new_model = MambaForCausalLM.from_pretrained(MODEL_PATH, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b5a88d-e9aa-458f-ade9-ff63b70b3e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(51200, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x MambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22e32abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Check out Saved Model  ======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n====== Check out Saved Model  ======\\n')\n",
    "\n",
    "prompt1 = \"\"\"카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을\"\"\"\n",
    "out1 = generate(new_model, tokenizer, device, prompt1)\n",
    "\n",
    "\n",
    "prompt2 = \"\"\"활동은 소설, 극작, 수필, 번역, 라디오 드라마와 영화 시나리오 등 다양하다. 1974년부터 1991년까지 공산당에 입당하고 있으며, 초기에는 막시즘 관점에서\"\"\"\n",
    "out2 = generate(new_model, tokenizer, device, prompt2)\n",
    "\n",
    "\n",
    "prompt3 = \"\"\"근육이 커지기 위해서는\"\"\"\n",
    "out3 = generate(new_model, tokenizer, device, prompt3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cfb94f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8615b4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['활동은 소설, 극작, 수필, 번역, 라디오 드라마와 영화 시나리오 등 다양하다. 1974년부터 1991년까지 공산당에 입당하고 있으며, 초기에는 막시즘 관점에서의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6344e4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['근육이 커지기 위해서는의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 의 ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9174a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9fc9c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model'\n",
    "model_name = 'komamba'\n",
    "\n",
    "if not os.path.isdir(os.path.join(model_path, model_name)):\n",
    "    os.mkdir(os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad6a7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695aa22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0222d2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Downloading trl-0.7.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from trl) (2.1.2)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from trl) (4.36.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from trl) (1.24.4)\n",
      "Requirement already satisfied: accelerate in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from trl) (0.26.1)\n",
      "Requirement already satisfied: datasets in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from trl) (2.16.1)\n",
      "Collecting tyro>=0.5.11 (from trl)\n",
      "  Downloading tyro-0.7.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from torch>=1.4.0->trl) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (0.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (4.66.1)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
      "  Downloading shtab-1.6.5-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: psutil in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from accelerate->trl) (5.9.7)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from datasets->trl) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from datasets->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from datasets->trl) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from datasets->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from datasets->trl) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from datasets->trl) (3.9.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from aiohttp->datasets->trl) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from aiohttp->datasets->trl) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from requests->transformers>=4.31.0->trl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from requests->transformers>=4.31.0->trl) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from requests->transformers>=4.31.0->trl) (2023.11.17)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from pandas->datasets->trl) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from pandas->datasets->trl) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/knpu/anaconda3/envs/test/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Downloading trl-0.7.10-py3-none-any.whl (150 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.7.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.6.5-py3-none-any.whl (13 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shtab, mdurl, docstring-parser, markdown-it-py, rich, tyro, trl\n",
      "Successfully installed docstring-parser-0.15 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.7.0 shtab-1.6.5 trl-0.7.10 tyro-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28132013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2bcd376d-2517-4e41-b748-0f382e1f35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eee68a2b-86c1-48f0-acbe-35f25a606ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b99bcd3-7158-446e-b86e-e4f7126e00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b389c72-46a8-4597-ac01-fa3acb679ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label', 'input'],\n",
       "        num_rows: 135000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label', 'input'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9e6f01f-4a1a-447f-b7ad-32111cbe1c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'document': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "057476c0-5e1b-4755-9945-59ba90e4419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "int2label = {0: '부정', 1: '긍정'}\n",
    "label2int = {'부정': 0, '긍정': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9405f6d-6cdf-4ba0-b4af-834bdb275ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'질문의 내용이 긍정적인지 부정적인지 답변해 (질문): 아 더빙.. 진짜 짜증나네요 목소리 (답): \"할아버지의 말\"'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"질문의 내용이 긍정적인지 부정적인지 답변해 (질문): 아 더빙.. 진짜 짜증나네요 목소리 (답):\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e63586c8-3de2-4193-a2bd-3a46714ef8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'질문의 내용이 긍정적인지 부정적인지 답변해 (질문): 너무재었다그래서보는것을추천한다 (답): \"직접적으로\"'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 너무재밓었다그래서보는것을추천한다\\n (답):\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fad234a6-7779-4f79-8c53-95377d5ab459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'질문의 내용이 긍정적인지 부정적인지 답변해 (질문): 울면서 손들고 횡단보도 건널때 뛰쳐나올뻔 이범수 연기 드럽게못해 (답): \"차라리 안 되니?\"'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 울면서 손들고 횡단보도 건널때 뛰쳐나올뻔 이범수 연기 드럽게못해\\n (답):\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c631c20d-6666-4957-bd5b-f0da34ea3718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'질문의 내용이 긍정적인지 부정적인지 답변해 (질문): 울면서 손들고 횡단보도 건널때 뛰쳐나올뻔 이범수 연기 드럽게못해 (답): \"차라리 안 되니?\"'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 울면서 손들고 횡단보도 건널때 뛰쳐나올뻔 이범수 연기 드럽게못해\\n (답):\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "46a0f8d8-cb04-4113-b306-ef16795759e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label', 'input'],\n",
       "    num_rows: 135000\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b3421da8-0cc5-4b68-9855-278b4cc69ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label', 'input'],\n",
       "    num_rows: 15000\n",
       "})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f6c29bf3-7ca1-45bb-99d9-0aca1cfd1043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c0ccba58-a18d-4a60-9d9d-376129a315bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367a6b34d65845aead81d4e71104fe76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ba76cbf8424f31a671995baedc419d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de731eafaabf41e88ac8accf1d1ee168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "import random\n",
    "\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "prompt_format1 = \"\"\"질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): %s\\n (답):%s\"\"\"\n",
    "prompt_format2 = \"\"\"다음 질문은 긍정일까요 부정일까요?\\n (질문): %s\\n (답):%s\"\"\"\n",
    "prompt_format3 = \"\"\"질문이 긍정적이야? 아니면 부정적이야?\\n (질문): %s \\n(답):%s\"\"\"\n",
    "\n",
    "prompts = [prompt_format1, prompt_format2, prompt_format3]\n",
    "def gen_prompt_nsmc(element):\n",
    "    #prompt_format = prompts[random.randint(0, len(prompts)-1)]\n",
    "    prompt_format = prompts[0]\n",
    "    return DatasetDict({'input': prompt_format%(element['document'], int2label[element['label']])})\n",
    "\n",
    "\n",
    "train_dataset['train'] = train_dataset['train'].map(gen_prompt_nsmc)\n",
    "train_dataset['test'] = train_dataset['test'].map(gen_prompt_nsmc)\n",
    "dataset['test'] = dataset['test'].map(gen_prompt_nsmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f0f2c9f3-51d7-416c-a691-4112bf09ced6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6049534',\n",
       " 'document': '다른시각으로서의테무진아쉬움투성이영화는영화라지만역사와크게틀리다',\n",
       " 'label': 0,\n",
       " 'input': '질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 다른시각으로서의테무진아쉬움투성이영화는영화라지만역사와크게틀리다\\n (답):부정'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e6b8736d-2fea-4eae-8f0e-58779dd33f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6049534</td>\n",
       "      <td>다른시각으로서의테무진아쉬움투성이영화는영화라지만역사와크게틀리다</td>\n",
       "      <td>0</td>\n",
       "      <td>질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 다른시각으로서의테무진아쉬움...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9066833</td>\n",
       "      <td>이건 대체 뭐하자는거지</td>\n",
       "      <td>0</td>\n",
       "      <td>질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 이건 대체 뭐하자는거지\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6708819</td>\n",
       "      <td>이게 뭐야...액션도 토하겠고...</td>\n",
       "      <td>0</td>\n",
       "      <td>질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 이게 뭐야...액션도 토하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9447159</td>\n",
       "      <td>나만당할수는없지ㅋㅋㅋ꼭보시길</td>\n",
       "      <td>1</td>\n",
       "      <td>질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 나만당할수는없지ㅋㅋㅋ꼭보시...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9943250</td>\n",
       "      <td>전당포는 안하시는 우주파괴급 파워 아저씨</td>\n",
       "      <td>1</td>\n",
       "      <td>질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 전당포는 안하시는 우주파괴...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134995</th>\n",
       "      <td>5555645</td>\n",
       "      <td>마지막 그 한마디의 북받침이란...</td>\n",
       "      <td>1</td>\n",
       "      <td>질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 마지막 그 한마디의 북받침...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134996</th>\n",
       "      <td>8694184</td>\n",
       "      <td>재미없습니다 그리고 신음 소리땜에 당황했네요 아 물런 전 21살 영화가 참 재미없어...</td>\n",
       "      <td>0</td>\n",
       "      <td>질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 재미없습니다 그리고 신음 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134997</th>\n",
       "      <td>3359765</td>\n",
       "      <td>이런 영화는 앞으로 만들지마라..배우들도 작품선택좀 잘하고...</td>\n",
       "      <td>0</td>\n",
       "      <td>질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 이런 영화는 앞으로 만들지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134998</th>\n",
       "      <td>4301777</td>\n",
       "      <td>로버트 패틴슨의 안습인 연기.... 내 달리를 망쳤어 ㅠㅜ</td>\n",
       "      <td>0</td>\n",
       "      <td>질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 로버트 패틴슨의 안습인 연...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134999</th>\n",
       "      <td>6888887</td>\n",
       "      <td>충격과 공포 해피앤딩이 아니더라도 저렇게 웃기게 끝내는건 시청하는 시청자들한테 실망...</td>\n",
       "      <td>0</td>\n",
       "      <td>질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 충격과 공포 해피앤딩이 아...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                           document  label  \\\n",
       "0       6049534                  다른시각으로서의테무진아쉬움투성이영화는영화라지만역사와크게틀리다      0   \n",
       "1       9066833                                       이건 대체 뭐하자는거지      0   \n",
       "2       6708819                                이게 뭐야...액션도 토하겠고...      0   \n",
       "3       9447159                                    나만당할수는없지ㅋㅋㅋ꼭보시길      1   \n",
       "4       9943250                             전당포는 안하시는 우주파괴급 파워 아저씨      1   \n",
       "...         ...                                                ...    ...   \n",
       "134995  5555645                                마지막 그 한마디의 북받침이란...      1   \n",
       "134996  8694184  재미없습니다 그리고 신음 소리땜에 당황했네요 아 물런 전 21살 영화가 참 재미없어...      0   \n",
       "134997  3359765                이런 영화는 앞으로 만들지마라..배우들도 작품선택좀 잘하고...      0   \n",
       "134998  4301777                   로버트 패틴슨의 안습인 연기.... 내 달리를 망쳤어 ㅠㅜ      0   \n",
       "134999  6888887  충격과 공포 해피앤딩이 아니더라도 저렇게 웃기게 끝내는건 시청하는 시청자들한테 실망...      0   \n",
       "\n",
       "                                                    input  \n",
       "0       질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 다른시각으로서의테무진아쉬움...  \n",
       "1       질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 이건 대체 뭐하자는거지\\n...  \n",
       "2       질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 이게 뭐야...액션도 토하...  \n",
       "3       질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 나만당할수는없지ㅋㅋㅋ꼭보시...  \n",
       "4       질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 전당포는 안하시는 우주파괴...  \n",
       "...                                                   ...  \n",
       "134995  질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 마지막 그 한마디의 북받침...  \n",
       "134996  질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 재미없습니다 그리고 신음 ...  \n",
       "134997  질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 이런 영화는 앞으로 만들지...  \n",
       "134998  질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 로버트 패틴슨의 안습인 연...  \n",
       "134999  질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 충격과 공포 해피앤딩이 아...  \n",
       "\n",
       "[135000 rows x 4 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0358a66b-c1f6-4efe-9872-10b9a91782ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2738397aa8a4374a90f46e7ba841435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364c0503620647ce931123cfa03d7020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 135000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    outputs = tokenizer(\n",
    "        element['input'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    return {\"input_ids\": outputs[\"input_ids\"]}\n",
    "\n",
    "\n",
    "context_length=128\n",
    "tokenized_datasets = train_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=train_dataset['train'].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "81a3c360-8dd9-442c-8e15-ac692f515f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fef312b6-d177-4c88-aee3-dfc67f15855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets['train'][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "01f85889-26b2-4324-a6ac-b1a41797df0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"kowikimamba\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    weight_decay=0.1,\n",
    "    #warmup_steps=logging_steps,\n",
    "    #max_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    save_steps=5000,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "trainer = MambaTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "99e7378c-78f4-423e-86e5-bf586c733355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='789' max='789' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [789/789 1:30:18, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=789, training_loss=0.17450283809608683, metrics={'train_runtime': 5428.1994, 'train_samples_per_second': 74.61, 'train_steps_per_second': 0.145, 'total_flos': 0.0, 'train_loss': 0.17450283809608683, 'epoch': 2.99})"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891e090-e310-43ac-adf0-e12b453e47ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "739ea7d7-04e5-459a-be06-94eb52a93fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'질문의 내용이 긍정적인지 부정적인지 답변해 (질문): 아 더빙.. 진짜 짜증나네요 목소리 (답):부정적으로 모든게 추천하고 싶은 영화..고맙습니다..내가정말좋은일도있고이쁜게있으니까고나왔으면좋겠는데..이런글남긴시작같고.답답해.이태재개막장..개재미없어 (답):부정잡아할때만보면아직도죽는줄..와.. (답):부정잡아님. (답):부정잡종'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"질문의 내용이 긍정적인지 부정적인지 답변해 (질문): 아 더빙.. 진짜 짜증나네요 목소리 (답):\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "ebd1b3dd-6b94-4fcc-92d7-9fd18ad73daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'질문의 내용이 긍정적인지 부정적인지 답변해 (질문): 너무재었다그래서보는것을추천한다 (답):긍정적인귀신이란사람들은고양이야.그래도솔직히강윤재너무불쌍하지않음??줄거다줬는데 진심으로~그거알면서도딴남자만나네?얼라들이이거보고잘배우겠네?이러닌깐여자들이여우라는소리듣는거야~졸라재미없어~안봐이제..작가누구야? (답):부정잡종아닌가요'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 너무재밓었다그래서보는것을추천한다\\n (답):\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "42a9ba91-6c45-4dd2-9517-9fe87d8cd94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'질문의 내용이 긍정적인지 부정적인지 답변해 (질문): 울면서 손들고 횡단보도 건널때 뛰쳐나올뻔 이범수 연기 드럽게못해 (답):부정 연기도 너무 좋았음. 그리고 개콘감들은 다 안쓰럽게 웃긴건 또 보고싶다는거냐.내가 사람한텐 할일없는사람이이이스토리를 이제 이렇게 잘만들면 이유가 뭘까...;;;; (답):긍정적인나저나 (답):긍정적인나저나 (답):긍정적인**************'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 울면서 손들고 횡단보도 건널때 뛰쳐나올뻔 이범수 연기 드럽게못해\\n (답):\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ceea27-6670-4254-85ce-a87266b353ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ff65dc14-3952-479f-9cc3-d606fb17cb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label', 'input'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "dc92f08a-bdcd-43da-b48c-76597f2233b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '9501525',\n",
       " 'document': '훌륭한 작가와 뛰어난 연출진이 만나, 훌륭하고 뛰어난 한 편의 드라마가 만들어졌다.',\n",
       " 'label': 1,\n",
       " 'input': '질문의 내용이 긍정적인지 부정적인지 답변해\\n (질문): 훌륭한 작가와 뛰어난 연출진이 만나, 훌륭하고 뛰어난 한 편의 드라마가 만들어졌다.\\n (답):긍정'}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'] [123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "4310a929-2791-4627-8bd3-745fab748d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset = dataset['test'].map(\n",
    "    tokenize, batched=True, remove_columns=['id', 'input', 'document']\n",
    ")\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b92b7616-cbc9-4b49-b660-439478e21920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=16\n",
    "val_ds = valid_dataset\n",
    "val_ds.set_format(type='torch')\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "0b388e74-5c24-41cb-a01c-3478bf007b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def acc(pred,label):\n",
    "    return torch.sum(torch.tensor(pred) == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "7e5f3b33-0b8b-49ae-a22c-4d2bd9ba3afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [31:57<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:  0.99988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_losses = []\n",
    "val_acc = 0\n",
    "\n",
    "for step, batch in enumerate(tqdm(val_dl)):\n",
    "    label = batch['label']\n",
    "    input_id= batch['input_ids'].to(device)\n",
    "\n",
    "    pred = model.generate(input_id, max_length=150)\n",
    "    decoded_pred = tokenizer.batch_decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    decoded_pred = [re.findall(\"\\(답\\):[부정|긍정]*\", x)[0][4:6] if re.findall(\"\\(답\\):[부정|긍정]*\", x) else 'none' for x in decoded_pred]\n",
    "    decoded_pred = [label2int[x] if x in label2int else -1 for x in decoded_pred]\n",
    "    val_acc += acc(decoded_pred, label)\n",
    "    \n",
    "\n",
    "print(\"val acc: \", val_acc/len(val_dl.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d5e25-0338-4158-9282-a4638264a730",
   "metadata": {},
   "source": [
    "(1) 790m(10000 steps), 3epochs, prompt#0, 0.99988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "cc050c05-cad7-4052-b956-45f3989b03f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49994"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36a66d-151e-459c-b363-2171f2b99dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8fe1aa-7ac1-41f4-aabd-2b155c20243c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa59990a-77a3-433a-857c-35b4c88fd6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.50346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def96ce0-bc7f-4077-b800-696504a8cce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c612add-be8c-4776-8f7e-25fba5257ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fae6cf-0156-4599-ab8e-b1850007647c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff234f-aa3b-4117-a6fc-60f0037beaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc110c-d4cf-406c-a28f-0867e5c21cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0519f5e-6e41-443a-b551-ef5c62ff4f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0c266-7167-422e-bd10-e89dc8b3cee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7bb4f-cb91-40f0-b4bb-1309d93e5198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aeb9f4-22ca-420c-bec8-1156577ecd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2aa09-60b5-4c32-8b27-26635ad2ed6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b601543-78fd-48a8-bfef-7b45b8d6a857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281ffb3-a0ca-4fb4-adfa-57b0d8d3ec86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ce87853-035e-4a87-85f6-186c7d928d7b",
   "metadata": {},
   "source": [
    "### KLUE NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "2ec75a32-5026-48b1-b640-2c598923460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_dataset = load_dataset('klue', 'nli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "375afc12-0c72-4cc9-81e3-ef9b8ac09c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['guid', 'source', 'premise', 'hypothesis', 'label'],\n",
       "        num_rows: 24998\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['guid', 'source', 'premise', 'hypothesis', 'label'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "a74113e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24998"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nli_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f444d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nli_dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "448c5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_train_dataset = nli_dataset[\"train\"].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "7a54a351-3a49-4417-908a-f7ff10830241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['guid', 'source', 'premise', 'hypothesis', 'label'],\n",
       "        num_rows: 22498\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['guid', 'source', 'premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "510bcae3-f750-4587-b4ce-47bc80cc5245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': Value(dtype='string', id=None),\n",
       " 'source': Value(dtype='string', id=None),\n",
       " 'premise': Value(dtype='string', id=None),\n",
       " 'hypothesis': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)}"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_train_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "9e621971-2341-48a3-b278-4e58298310fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': Value(dtype='string', id=None),\n",
       " 'source': Value(dtype='string', id=None),\n",
       " 'premise': Value(dtype='string', id=None),\n",
       " 'hypothesis': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)}"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_dataset['validation'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "5fa85c93-7945-4ac4-8607-7716cb03db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "int2label = {0: '함의', 1: '중립', 2: '모순'}\n",
    "label2int = {'함의': 0, '중립': 1, '모순':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "3cd06e60-3335-4943-830f-dff14957b180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9510d95e61cb42f0ac6eb3b4905b0167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6810085b414df7aba8ce82292db2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "import random\n",
    "\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "prompt_format1 = \"\"\"다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1): %s\\n (문장2): %s\\n (답):%s\"\"\"\n",
    "prompt_format2 = \"\"\"다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1): %s\\n (문장2): %s\\n (답):%s\"\"\"\n",
    "prompt_format3 = \"\"\"다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1): %s\\n (문장2): %s\\n (답):%s\"\"\"\n",
    "\n",
    "prompts = [prompt_format1, prompt_format2, prompt_format3]\n",
    "def gen_prompt_nli(element):\n",
    "    #prompt_format = prompts[random.randint(0, len(prompts)-1)]\n",
    "    prompt_format = prompts[0]\n",
    "    return DatasetDict({'input': prompt_format%(element['premise'], element['hypothesis'], int2label[element['label']])})\n",
    "\n",
    "\n",
    "nli_train_dataset['train'] = nli_train_dataset['train'].map(gen_prompt_nli)\n",
    "nli_train_dataset['test'] = nli_train_dataset['test'].map(gen_prompt_nli)\n",
    "nli_dataset['validation'] = nli_dataset['validation'].map(gen_prompt_nli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "1b2a6240-3485-45cd-96aa-6eaa70b225e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'klue-nli-v1_train_20786',\n",
       " 'source': 'airbnb',\n",
       " 'premise': '집주인이 너무 친절하며 답장도 굉장히 빠릅니다.',\n",
       " 'hypothesis': '집주인의 답장은 천천히 옵니다.',\n",
       " 'label': 2,\n",
       " 'input': '다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1): 집주인이 너무 친절하며 답장도 굉장히 빠릅니다.\\n (문장2): 집주인의 답장은 천천히 옵니다.\\n (답):모순'}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_train_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "eb14f016-2b2a-4159-9855-eba94e391d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>source</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klue-nli-v1_train_20786</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>집주인이 너무 친절하며 답장도 굉장히 빠릅니다.</td>\n",
       "      <td>집주인의 답장은 천천히 옵니다.</td>\n",
       "      <td>2</td>\n",
       "      <td>다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>klue-nli-v1_train_17382</td>\n",
       "      <td>wikitree</td>\n",
       "      <td>이외에도 콘크리트 발판을 추가하거나 자갈을 깔 수도 있다.</td>\n",
       "      <td>콘크리트 발판을 더하거나 작은 돌들을 놓을 수도 있다.</td>\n",
       "      <td>0</td>\n",
       "      <td>다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>klue-nli-v1_train_16901</td>\n",
       "      <td>wikitree</td>\n",
       "      <td>이번 협약은 세 기관 간 해외투자유치 협력체계를 구축하고 이를 통한 시너지 창출 및...</td>\n",
       "      <td>이번 협약의 세 기관 모두 투자증권 회사이다.</td>\n",
       "      <td>1</td>\n",
       "      <td>다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klue-nli-v1_train_05101</td>\n",
       "      <td>NSMC</td>\n",
       "      <td>누구라도 손에 꼽을 사상최고의 걸작만화.</td>\n",
       "      <td>최고의 걸작 만화로 뽑힌 걸작.</td>\n",
       "      <td>1</td>\n",
       "      <td>다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klue-nli-v1_train_07914</td>\n",
       "      <td>policy</td>\n",
       "      <td>먼저 중소기업과 소상공인 제품을 중심으로 민간 쇼핑몰, 가치삽시다 플랫폼 등을 통해...</td>\n",
       "      <td>먼저 중소기업과 소상공인 제품을 중심으로 민간 쇼핑몰 등을 통해 오프라인 방식의 판...</td>\n",
       "      <td>2</td>\n",
       "      <td>다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22493</th>\n",
       "      <td>klue-nli-v1_train_02189</td>\n",
       "      <td>wikitree</td>\n",
       "      <td>공예명장관에서는 송현경 대한민국 명장을 포함한 한경희 광주 공예명장 등의 작품 20...</td>\n",
       "      <td>공예명장관에서 명장들의 작품을 보여줄 예정이다.</td>\n",
       "      <td>0</td>\n",
       "      <td>다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22494</th>\n",
       "      <td>klue-nli-v1_train_04802</td>\n",
       "      <td>NSMC</td>\n",
       "      <td>내가 영화론과 예술을이정도로 잊고있었던건가.</td>\n",
       "      <td>많은 것을 잊고 있었다.</td>\n",
       "      <td>1</td>\n",
       "      <td>다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>klue-nli-v1_train_05668</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>당시 바그너는 스위스 루체른의 교외에 있는 트리프센에 거주하면서 작곡과 논문을 집필...</td>\n",
       "      <td>바그너는 트리프센에 거주했다.</td>\n",
       "      <td>0</td>\n",
       "      <td>다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>klue-nli-v1_train_22113</td>\n",
       "      <td>policy</td>\n",
       "      <td>태안 해수욕장 중 유일하게 입장료가 있는 해수욕장이며 입장료 안에는 주차료, 샤워비...</td>\n",
       "      <td>해수욕장의 입장료는 일인당 15000원 이다.</td>\n",
       "      <td>1</td>\n",
       "      <td>다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>klue-nli-v1_train_09864</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>사고 발생 즉시 2호선 전 열차의 운행이 중지됐으며, 승객에게 다른 교통수단 이용을...</td>\n",
       "      <td>사고 발생 후 승객에게 열차를 떠나지 말라는 안내방송이 있었다.</td>\n",
       "      <td>2</td>\n",
       "      <td>다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22498 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          guid     source  \\\n",
       "0      klue-nli-v1_train_20786     airbnb   \n",
       "1      klue-nli-v1_train_17382   wikitree   \n",
       "2      klue-nli-v1_train_16901   wikitree   \n",
       "3      klue-nli-v1_train_05101       NSMC   \n",
       "4      klue-nli-v1_train_07914     policy   \n",
       "...                        ...        ...   \n",
       "22493  klue-nli-v1_train_02189   wikitree   \n",
       "22494  klue-nli-v1_train_04802       NSMC   \n",
       "22495  klue-nli-v1_train_05668  wikipedia   \n",
       "22496  klue-nli-v1_train_22113     policy   \n",
       "22497  klue-nli-v1_train_09864   wikinews   \n",
       "\n",
       "                                                 premise  \\\n",
       "0                             집주인이 너무 친절하며 답장도 굉장히 빠릅니다.   \n",
       "1                       이외에도 콘크리트 발판을 추가하거나 자갈을 깔 수도 있다.   \n",
       "2      이번 협약은 세 기관 간 해외투자유치 협력체계를 구축하고 이를 통한 시너지 창출 및...   \n",
       "3                                 누구라도 손에 꼽을 사상최고의 걸작만화.   \n",
       "4      먼저 중소기업과 소상공인 제품을 중심으로 민간 쇼핑몰, 가치삽시다 플랫폼 등을 통해...   \n",
       "...                                                  ...   \n",
       "22493  공예명장관에서는 송현경 대한민국 명장을 포함한 한경희 광주 공예명장 등의 작품 20...   \n",
       "22494                           내가 영화론과 예술을이정도로 잊고있었던건가.   \n",
       "22495  당시 바그너는 스위스 루체른의 교외에 있는 트리프센에 거주하면서 작곡과 논문을 집필...   \n",
       "22496  태안 해수욕장 중 유일하게 입장료가 있는 해수욕장이며 입장료 안에는 주차료, 샤워비...   \n",
       "22497  사고 발생 즉시 2호선 전 열차의 운행이 중지됐으며, 승객에게 다른 교통수단 이용을...   \n",
       "\n",
       "                                              hypothesis  label  \\\n",
       "0                                      집주인의 답장은 천천히 옵니다.      2   \n",
       "1                         콘크리트 발판을 더하거나 작은 돌들을 놓을 수도 있다.      0   \n",
       "2                              이번 협약의 세 기관 모두 투자증권 회사이다.      1   \n",
       "3                                      최고의 걸작 만화로 뽑힌 걸작.      1   \n",
       "4      먼저 중소기업과 소상공인 제품을 중심으로 민간 쇼핑몰 등을 통해 오프라인 방식의 판...      2   \n",
       "...                                                  ...    ...   \n",
       "22493                         공예명장관에서 명장들의 작품을 보여줄 예정이다.      0   \n",
       "22494                                      많은 것을 잊고 있었다.      1   \n",
       "22495                                   바그너는 트리프센에 거주했다.      0   \n",
       "22496                          해수욕장의 입장료는 일인당 15000원 이다.      1   \n",
       "22497                사고 발생 후 승객에게 열차를 떠나지 말라는 안내방송이 있었다.      2   \n",
       "\n",
       "                                                   input  \n",
       "0      다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...  \n",
       "1      다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...  \n",
       "2      다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...  \n",
       "3      다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...  \n",
       "4      다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...  \n",
       "...                                                  ...  \n",
       "22493  다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...  \n",
       "22494  다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...  \n",
       "22495  다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...  \n",
       "22496  다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...  \n",
       "22497  다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1...  \n",
       "\n",
       "[22498 rows x 6 columns]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_train_dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "1f71838e-aafa-4812-a5c3-c08f63ee7517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f70c66e6c1470e80873e9863f2528c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 22498\n",
       "})"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    outputs = tokenizer(\n",
    "        element['input'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    return {\"input_ids\": outputs[\"input_ids\"]}\n",
    "\n",
    "\n",
    "context_length=128\n",
    "tokenized_nli_datasets = nli_train_dataset['train'].map(\n",
    "    tokenize, batched=True, remove_columns=nli_train_dataset['train'].column_names\n",
    ")\n",
    "tokenized_nli_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "715a3995-81d3-4e66-a16a-fc5cb4095e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c4e46f4652468eae0102e90bcb1bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_nli_validation_datasets = nli_dataset['validation'].map(\n",
    "    tokenize, batched=True, remove_columns=nli_dataset['validation'].column_names\n",
    ")\n",
    "tokenized_nli_validation_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "076bcb59-e4b0-4e14-ab93-640146060e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "c9bb6051-7332-49d6-a16c-a43cd954b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets['train'][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "cd4a8932-145d-4507-8248-941dcf1c0539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"kowikimamba\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    weight_decay=0.1,\n",
    "    #warmup_steps=logging_steps,\n",
    "    #max_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    save_steps=5000,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "trainer = MambaTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_nli_datasets,\n",
    "    eval_dataset=tokenized_nli_validation_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "356df19b-33c7-4fc4-ada2-5527e5d7ac56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 14:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=132, training_loss=0.2015042593984893, metrics={'train_runtime': 907.0973, 'train_samples_per_second': 74.407, 'train_steps_per_second': 0.146, 'total_flos': 0.0, 'train_loss': 0.2015042593984893, 'epoch': 3.0})"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "7f9da68d-6e19-44d9-9aa7-2be610425428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘. (문장1): 10년전 나를 매료시켰던 묘하고 멋진영화 (문장2): 10년 전에 나를 실망시켰던 영화 (답):모순한사랑의 영화. (문장2): 10년 전에 나를 실망시켰던 영화. (답):모순한사랑의 사랑과 가족. (답):모순한사랑의 행복한 인생이라는 느낌은 너무도 먹먹하다. (답):중립적인 사랑을 보여주는 영화. (답):모순하고 있다. (답):중립적인 분위기가 오래도록 살아있다. (답):중립적인 분위기가 가장 잘 드러나는'"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1): 10년전 나를 매료시켰던 묘하고 멋진영화\\n (문장2): 10년 전에 나를 실망시켰던 영화\\n (답):\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "995376a0-6f63-4c64-b79a-14a9d69a274e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘. (문장1): 10여 년 만에 출소한 금고 털이범 아버지 정도의 등장으로 아들 봉수와 며느리는 놀란다. (문장2): 정도는 감옥에서 모범수였다. (답):중립으로 인한 범죄를 저질렀다. (답):모순희 씨의 개인 취권은 서울 강남구 봉은사로에 위치한 바비레드 강남점에 인앤아웃 버거 팝업스토어가 열렸다. (답):모순하우는 서울 강남의 일드인게 아닐까 한다. (답):모순한모순유순가어이가없어요. (답):모순하얼은중립적 정신은 뭘까? (답):모'"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1): 10여 년 만에 출소한 금고 털이범 아버지 정도의 등장으로 아들 봉수와 며느리는 놀란다.\\n (문장2): 정도는 감옥에서 모범수였다.\\n (답):\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "a2bd649d-55be-43f1-a625-f942e8cbe84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘. (문장1): 10점 만점에 10점을 줘도 부족한 드라마입니다. (문장2): 드라마의 내용이 만점입니다. (답):모순간 후만 못합니다. (문장2): 이 드라마는 10점 만점을 넘는 점수를 주고 싶은 드라마입니다. (답):중립적인 프로그램을 진행하며 전시에 필요한 내용을 편성했습니다. (답):모순적으로 제작시간이 더 많은 편이었습니다. (답):중립될 계획입니다. (답):모순적으로 제작시간이 더 비싸지 않았습니다. (답):중립될 예정입니다. (답):중립될'"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"다음 문장1과 문장2의 관계를 함의, 중립, 모순 중 하나로 분류해줘.\\n (문장1): 10점 만점에 10점을 줘도 부족한 드라마입니다.\\n (문장2): 드라마의 내용이 만점입니다.\\n (답):\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device=device)\n",
    "attn_mask = inputs.attention_mask.to(device=device)\n",
    "max_length = input_ids.shape[1] + 100\n",
    "\n",
    "\n",
    "# Generate\n",
    "fn = lambda: model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    cg=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    ")\n",
    "\n",
    "out = fn()\n",
    "tokenizer.batch_decode(out.sequences.tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "78134752-f1b2-452f-9732-321764fedfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['guid', 'source', 'premise', 'hypothesis', 'label', 'input'],\n",
       "    num_rows: 2500\n",
       "})"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_train_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "2b5fde70-f960-46bf-807c-65af0003029c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94dfbf044b374ea08f922dabb54223bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids'],\n",
       "    num_rows: 2500\n",
       "})"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_valid_dataset = nli_train_dataset['test'].map(\n",
    "    tokenize, batched=True, remove_columns=['guid', 'source', 'premise', 'hypothesis', 'input']\n",
    ")\n",
    "nli_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "4c910768-e2de-46ba-af4f-e71d199c1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=16\n",
    "nli_val_ds = nli_valid_dataset\n",
    "nli_val_ds.set_format(type='torch')\n",
    "nli_val_dl = DataLoader(nli_val_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "102438c9-46aa-4874-8581-c971a5add6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def acc(pred,label):\n",
    "    return torch.sum(torch.tensor(pred) == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "a0b1533f-0b39-4b2b-ad3a-967666644154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [01:37<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_losses = []\n",
    "val_acc = 0\n",
    "\n",
    "for step, batch in enumerate(tqdm(nli_val_dl)):\n",
    "    label = batch['label']\n",
    "    input_id= batch['input_ids'].to(device)\n",
    "\n",
    "    pred = model.generate(input_id, max_length=150)\n",
    "    decoded_pred = tokenizer.batch_decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    decoded_pred = [re.findall(\"\\(답\\):[함의|중립|모순]*\", x)[0][4:6] if re.findall(\"\\(답\\):[함의|중립|모순]*\", x) else 'none' for x in decoded_pred]\n",
    "    decoded_pred = [label2int[x] if x in label2int else -1 for x in decoded_pred]\n",
    "    val_acc += acc(decoded_pred, label)\n",
    "    \n",
    "print(\"val acc: \", val_acc/len(nli_val_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "2d3c5ae6-655e-4210-ac07-eeb34285430c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "9d098751-a961-462b-9ae9-99e15e465d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nli_val_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e8ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
